% !TEX program = pdflatex
% !TEX enableSynctex = true
\documentclass[aspectratio=169,10pt]{beamer}

%%%%%%%%%%%%%%%
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{ulem}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[capitalise,noabbrev]{cleveref} 
\usepackage{datatool}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{empheq}
\usepackage[many]{tcolorbox}
\usepackage[capitalise,noabbrev]{cleveref}
\usetheme[progressbar=frametitle,block=fill]{metropolis}

\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
\definecolor{emphcolorval}{rgb}{0.23,0.4,0.7}
\definecolor{highlightcolorval}{rgb}{1,0,0}
% Penn colors
\definecolor{PennRed}{RGB}{149, 0, 26}
\definecolor{PennBlue}{RGB}{1, 37, 110}

\setbeamercolor{frametitle}{bg=PennBlue}
\setbeamercolor{progress bar}{fg=PennBlue}
\setbeamercolor{math text}{fg=PennRed}

\setbeamercolor{block title}{fg=PennBlue}
\setbeamercolor{block body}{fg=PennBlue}

\setbeamercolor{block title example}{fg=PennBlue}
\setbeamercolor{block body example}{fg=PennBlue, bg=yellow}

\setbeamersize{text margin left=5.5pt, text margin right=5.5pt}

\definecolor{cosmiclatte}{rgb}{1.0, 0.99, 0.95}
\setbeamercolor{background canvas}{bg=cosmiclatte}
%\titlegraphic{\hfill\includegraphics[height=0.8cm]{/Users/jesusfv/dropbox/Templates_Slides/penn_fulllogo.pdf}}
\newcommand{\emphcolor}[1]{\textbf{\textcolor{emphcolorval}{#1}}}
%\newcommand{\mathcolor}[1]{{\mathbf{\color{emphcolorval}{#1}}}}
\newcommand{\highlightcolor}[1]{{\textbf{\color{highlightcolorval}{#1}}}}

% \usepackage{natbib}
% \bibliographystyle{ecta}

\crefname{equation}{}{}
\newtheorem{proposition}{Proposition}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}
\newcommand{\D}[1][]{\ensuremath{\boldsymbol{\partial}_{#1}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\diff}{\ensuremath{\mathrm{d}}}
\newcommand{\ex}{\ensuremath{\mathrm{ex}}}
\newcommand{\set}[1]{\ensuremath{\left\{{#1}\right\}}}
\newcommand{\indicator}[1]{\ensuremath{\mathds{1}\left\{{#1}\right\}}}
\newcommand{\condexpec}[3][]{\ensuremath{\mathbb{E}_{#1}\left[{#2} \; \middle| \; {#3} \right]}}
\newcommand{\prob}[2][]{\ensuremath{\mathbb{P}_{#1}\left( {#2} \right)}}
\newcommand{\cprob}[2]{\ensuremath{\mathbb{P}\left( {#1}\left| {#2} \right. \right)}}
\newcommand{\condcov}[2]{\ensuremath{\mathrm{cov}\left({#1} \; \middle| \; {#2} \right)}}
\newcommand{\expec}[2][]{\ensuremath{\mathbb{E}_{{#1}}\left[ {#2} \right]}}
\newcommand{\bigO}[1]{\ensuremath{\mathcal{O}(#1)}}
\newcommand{\Xdom}{\mathcal{X}}
\newcommand{\Yrange}{\mathcal{Y}}
\newcommand{\Xtrain}{\mathcal{X}_{\mathrm{train}}}
\newcommand{\Xextr}{\mathcal{X}_{\mathrm{extr}}}
\newcommand{\Xhull}{\mathrm{Hull}(\Xtrain)}
\newcommand{\Xtest}{\mathcal{X}_{\mathrm{test}}}
\newcommand{\Ltest}{\ell_{\mathrm{test}}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Resid}{\mathcal{R}}
\newcommand{\st}{\textrm{s.t.}\,}
\usepackage{bm}

%bold letters
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\begin{document}
\title{{\vspace{0.4in}\hspace{0.2in}\textcolor{PennBlue}{Solving Models of Economic Dynamics with \\\quad Ridgeless Kernel Regressions}}}
\author{\hspace{0.2in}Mahdi Ebrahimi Kahou\inst{1}  \and Jesse Perla\inst{2} \and Geoff Pleiss\inst{3,}\inst{4}}
\institute{
	\inst{\hspace{0.2in}1}Bowdoin College, Econ Dept\and
	\inst{\hspace{0.2in}2}University of British Columbia, Vancouver School of Economics \and
	\inst{\hspace{0.2in}3} University of British Columbia, Stats Dept \and 
	\inst{\hspace{0.2in}4} Vector Institute
}

\date{\hspace{0.2in}
	ASSA 2026}

\maketitle

\begin{frame}{Motivation}
	\begin{itemize}
		\item Numerical solutions to dynamical systems are central to many quantitative fields in economics.
		% such as macroeconomics, empirical Industrial organization, urban economics, and spatial economics
		\vspace{0.1in}
		\item Dynamical systems in economics are \emphcolor{boundary value} problems:
		%unlike mnay problems natural sciences and engineering :
		\vspace{0.1in}
		\begin{enumerate}
				\item The boundary is at \emphcolor{infinity}.
				\vspace{0.05in}
				\item The values at the boundary are potentially \emphcolor{unknown}.
				\vspace{0.05in}  
		\end{enumerate}
		\item Resulting from \emphcolor{forward looking} behavior of agents.
		% and it is requirement for optimality
		\vspace{0.1in}
		\item Examples include the transversality and the no-bubble condition.
		% No ponzi scheme
		\vspace{0.1in}
		\item Without them, the problems are \emphcolor{ill-posed}  and have infinitely many solutions: 
		% ill-posed in the Hadamard sense, the solutions are not unique
		\vspace{0.1in}
		\begin{itemize}
			\item These forward-looking boundary conditions are a key limitation on increasing dimensionality.
			% what do I mean by that, if it wasnt a boundary condition at infinity, I could easily solve a 1000 dimensional initial value problem 
		\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}{Contribution}
 Using kernel method to solve a broad class of infinite-horizon, deterministic, continuous-time model 
\begin{enumerate}

	\item \emphcolor{Minimum-norm alignment}:
	\begin{itemize}
		\item The minimum-norm kernel method aligns with asymptotic boundary conditions.
	\end{itemize}
	\vspace{0.1in}
	\item \emphcolor{Learning the right set of steady-states}:
	\begin{itemize}
		\item Kernel machines learn the boundary values, thereby extrapolating outside the training data.
	\end{itemize}
	\vspace{0.1in}
	\item \emphcolor{Robustness and speed}:
	\begin{itemize}
		\item Competitive in speed and more stable than traditional methods.
		%trditional like: shooting
	\end{itemize}
	\vspace{0.1in}
	\item \emphcolor{Consistency of ML estimates}.
	% for the case of the kernels convergece to the true minimum-norm solutions as the data size goes to infinity
\end{enumerate}

\end{frame}


\begin{frame}{Intuition}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			% Content for the left column
			\begin{itemize}
				\item \emphcolor{Violation of the boundary conditions}:
				\begin{itemize}
					\item Sub-optimal solutions  explode over time.
					% As you can see in the plot optimal solution goes to the steady-state, the black dot, and any othe trajectory globaly or locally diveges. That is the signature of economic models
					\vspace{0.05in}
					\item They have large derivatives.
					% Given that they are divergent, then they are large functions with large norms
					\vspace{0.05in}
					\item This behavior is due to the \emphcolor{saddle-path} nature of the problem.
					% many economic models have this saddle path nature
				\end{itemize}
				\item \emphcolor{Minimum-norm solution }: 
				\begin{itemize}
					\item Penalizing large derivatives rules out explosive paths.
					%therefore they are many possible solutions
					\vspace{0.05in}
					\item The remaining solution is the optimal solution. 
					% so they dont doesnt like explosive functions 
				\end{itemize}
				\vspace{0.1in}
				
			\end{itemize}
		\end{column}
		\begin{column}{0.5\textwidth}
		\begin{figure}[t!]
			\centering
			\includegraphics[width=\textwidth]{figs/saddle_path_mu_k.pdf}
			\vspace{-7mm}
		\end{figure}
		\end{column}
	\end{columns}
\end{frame}

\section{The Problem}

\begin{frame}{The class of problems}
A differential-algebraic system of equations, coming from an optimization problem:	
	\begin{align}
		\dot{\bx}(t) &= \bF(\bx(t),\bmu(t)
		,\by(t))\\ %i.e. law of motions for states, like capital
		\dot{\bmu}(t) &= r \bmu(t)
		- \bmu(t)
		\odot \bG(\bx(t), \bmu(t)
		, \by(t))\\
		% coming from the optimizing behaviour, like Euler equations
		\mathbf{0} &= \bH(\bx(t), \bmu(t), \by(t)),
		% Usually comes from economics consideratios, like market clearing, no-arbitrage conditions. This can be thought of as what sets us aprat from ususal optimal control theory
	\end{align}
	boundary conditions (at infinity)
	\begin{align}
		\mathbf{0} = \lim_{t\rightarrow\infty} e^{-rt} \bx(t)\odot\bmu(t),
	\end{align}
	initial value $\bx(0) = \bx_0$.
	\begin{itemize}
		\item $\bx\in \mathbb{R}^{M}$: state variables.
		\item $\bmu \in \mathbb{R}^M$: co-state variables.
		\item $\mathbf{y}\in\mathbb{R}^{P}$: jump variables.
	\end{itemize}
\end{frame}

\begin{frame}{Challenges}
	\emphcolor{Goal}: Find an approximation to $\bx(t)$, $\bmu(t)$, and $\by(t)$.\\
	\vspace{0.1in}
	
	\emphcolor{What is the problem?}
	\begin{itemize}
		\item Initial conditions $\by_0$ and $\bmu_0$ are unknown.
		\vspace{0.1in}
		
		\item The optimal solution follows a \emphcolor{saddle path}.
		\vspace{0.1in}
		\begin{itemize}
			\item If $T$ is small, solutions are inaccurate due to premature enforcement of the steady state.
			\vspace{0.1in}
			\item If $T$ is large, the algorithms become increasingly numerically unstable.
		\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}{Example: Neoclassical Growth Model}
	\[
	\begin{aligned}
		\dot{x}(t) &= f(x(t)) - \delta x(t) - y(t)
		:= F(x(t), \mu(t), y(t)) \\[4pt]
		\dot{\mu}(t) &= r \mu(t) - \mu(t)
		\underbrace{\big(f'(x(t)) - \delta\big)}_{:= G(x(t), \mu(t), y(t))} \\[4pt]
		0 &= \mu(t)y(t) - 1
		:= H(x(t), \mu(t), y(t)) \\[6pt]
		x(0) &= x_0, \qquad
		\lim_{t \to \infty} e^{-rt} \mu(t) x(t) = 0
	\end{aligned}
	\]
	capital $x(t)$, consumption $y(t)$, utility $\log(y)$, present-value co-state variable $\mu(t)$, discount rate $r > 0$, depreciation rate $0 < \delta < 1$, and production function
	$f(x)$.
\end{frame}


\section{Method}

\begin{frame}{Method: approximation}
	\label{kernel-approax}
	\begin{itemize}
		\item Pick a set of points $\mathcal{D}\equiv \{t_1,\cdots,t_N\}$ for some fixed interval $[0,T]$
		\vspace{0.1in}
		\[
		\begin{aligned}
			\hat{\dot{\bx}}(t) &= \sum_{j=1}^N \balpha^x_j\, k(t,t_j), \qquad
			\hat{\dot{\bmu}}(t) = \sum_{j=1}^N \balpha^\mu_j\, k(t,t_j), \qquad
			\hat{\dot{\by}}(t) = \sum_{j=1}^N \balpha^y_j\, k(t,t_j), \\[6pt]
			\hat{\bx}(t) &= \bx_0 + \int_0^t \hat{\dot{\bx}}(\tau)\, d\tau, \qquad
			\hat{\bmu}(t) = \bmu_0 + \int_0^t \hat{\dot{\bmu}}(\tau)\, d\tau, \qquad
			\hat{\by}(t) = \by_0 + \int_0^t \hat{\dot{\by}}(\tau)\, d\tau.
		\end{aligned}
		\]
	% Essentially I turn the problem inot an empirical risk minimization style problem 
	\item $\balpha^x_j$, $\balpha^\mu_j$, $\balpha^y_j$, $\bmu_0$, and $\by_0$ are parameters to be found.
	 \vspace{0.1in}
	\item $k(t,t_j)$ is a kernel that measures  		``similarity" between $t$ and $t_j$.
	\vspace{0.1in}
	\item We use a Matérn kernel with smoothness $\nu$ and length $\ell$. \\ \hyperlink{Maten-def}{\beamerskipbutton{Matérn kernel}}
	\vspace{0.1in}
	\end{itemize}
\end{frame}

\begin{frame}{Method: Ridgeless kernel regression}
We solve 
\[
\begin{aligned}
	\min_{\substack{
			\hat{\dot{\bx}}, \hat{\dot{\bmu}},\hat{\dot{\by}}
	}}\, & \left(
	\sum_{m=1}^{M}\, \Vert \hat{\dot{\bx}}^{(m)} \Vert^2_{\mathcal{H}} +
	\sum_{m=1}^{M}\, \Vert \hat{\dot{\bmu}}^{(m)} \Vert^2_{\mathcal{H}}\right)\\
	\text{s.t.}~\,
	& \hat{\dot{\bx}}(t_i) = \bF(\hat{\bx}(t_i), \hat{\bmu}(t_i), \hat{\by}(t_i)),\quad\text{for all } t_i \in \mathcal{D}\\
	& \hat{\dot{\bmu}}(t_i) = r\hat{\bmu}(t_i)- \hat{\bmu}(t_i)\odot\bG(\hat{\bx}(t_i), \hat{\bmu}(t_i), \hat{\by}(t_i)),\quad\text{for all } t_i \in \mathcal{D}\\
	& \mathbf{0} = \bH(\hat{\bx}(t_i), \hat{\bmu}(t_i), \hat{\by}(t_i)),\quad\text{for all } t_i \in \mathcal{D}.
	\end{aligned}
\]
\begin{itemize}
	\item $\Vert \hat{\dot{\bx}}^{(m)} \Vert^2_{\mathcal{H}} = \textstyle\sum\limits_{i=1}^N \textstyle\sum\limits_{j=1}^N \alpha^{x^{(m)}}_i \, \alpha^{x^{(m)}}_j \, k(t_i, t_j)$ and $\Vert \hat{\dot{\bmu}}^{(m)} \Vert^2_{\mathcal{H}} = \textstyle\sum\limits_{i=1}^N \textstyle\sum\limits_{j=1}^N \alpha^{\mu^{(m)}}_i \, \alpha^{\mu^{(m)}}_j \, k(t_i, t_j)$
	\vspace{0.1in}
	\item TVC is \emphcolor{not imposed}.
		\vspace{0.1in}
	\item The penalization is used to control the smoothness of the approximating functions.
	\vspace{0.1in}
	\item For Matérn kernels, it also controls the smoothness of derivatives. 
\end{itemize}
\end{frame}

\section{Applications}

\begin{frame}{Neoclassical growth model}
	\[
	\begin{aligned}
		\max_{y(t)} &\int_0^\infty e^{-rt}\ln(y(t))dt\\
		& \text{s.t.} \quad \dot{x}(t) = f(x(t)) - y(t)-\delta x(t)
	\end{aligned}
	\]
for a given $x_0$.
\begin{itemize}
	\item $x(t)\in \mathbb{R}$: capital, $y(t)\in \mathbb{R}$: consumption, and a concave production function $f(x) = x^a$.
	\vspace{0.1in}
\end{itemize}

Constructing the Hamiltonian ...
	\[
	\begin{aligned}
		\dot{x}(t) &= f(x(t)) - \delta x(t) - y(t),\\
		\dot{\mu}(t) &= r \mu(t)  - \mu(t)
		\big(f'(x(t)) - \delta\big), \\
		0 &= \mu(t)y(t) - 1, \\
		x(0) &= x_0, \qquad
		\lim_{t \to \infty} e^{-rt} \mu(t) x(t) = 0.
	\end{aligned}
	\]
	\begin{itemize}
		\item Last Equation : transversality condition (TVC)
	\end{itemize}
\end{frame}

\begin{frame}{Why do we need the boundary condition?}
	Ignoring the transversality condition:
\[
\begin{aligned}
	\dot{x}(t) &= f(x(t)) - \delta x(t) - y(t),\\
	\dot{\mu}(t) &= r \mu(t)  - \mu(t)
	\big(f'(x(t)) - \delta\big), \\
	0 &= \mu(t)y(t) - 1, \\
	x(0) &= x_0.
\end{aligned}
\]
	\begin{figure}[t!]
		\centering
		\vspace{-4mm}
		\includegraphics[width=0.70\textwidth]{figs/TVC_violation.pdf}
		\vspace{-4mm}
	\end{figure}
\end{frame}

\begin{frame}{Neoclassical Growth Model: algorithm}
\[
\begin{aligned}
	\min_{\substack{
			\hat{\dot{x}}, \hat{\dot{\mu}},\hat{\dot{y}}
	}}\, & \left(
	 \Vert \hat{\dot{x}} \Vert^2_{\mathcal{H}} +
	 \Vert \hat{\dot{\mu}} \Vert^2_{\mathcal{H}}\right)\\
	\text{s.t.}~\,
	& \hat{\dot{x}}(t_i) = f(\hat{x}(t_i))- \delta \hat{x}(t_i) - \hat{y}(t_i) ,\quad\text{for all } t_i \in \mathcal{D}\\
	& \hat{\dot{\mu}}(t_i) = r\hat{\mu}(t_i)- \hat{\mu}(t_i)\big(f'(\hat{x}(t_i)-\delta)\big),\quad\text{for all } t_i \in \mathcal{D}\\
	& 0 = \hat{\mu}(t_i)\hat{y}(t_i) -1 ,\quad\text{for all } t_i \in \mathcal{D}.
\end{aligned}
\]
\begin{itemize}
	\item  $\Vert \hat{\dot{x}} \Vert^2_{\mathcal{H}} = \textstyle\sum\limits_{i=1}^N \textstyle\sum\limits_{j=1}^N \alpha^{x}_i \, \alpha^{x}_j \, k(t_i, t_j)$ and $\Vert \hat{\dot{\mu}} \Vert^2_{\mathcal{H}} = \textstyle\sum\limits_{i=1}^N \textstyle\sum\limits_{j=1}^N \alpha^{\mu}_i \, \alpha^{\mu}_j \, k(t_i, t_j)$
	\item TVC is \emphcolor{not imposed}.
\end{itemize}
\end{frame}


\begin{frame}{Neoclassical growth model: results}
	\label{ncg-results} 
	\begin{columns}
		\begin{column}{0.5\textwidth}
			% Content for the left column
				\begin{itemize}
					\item $\mathcal{D} = \{0,1,\cdots,40\}$.
					\hyperlink{sparse-sampling}{\beamerskipbutton{sparse grids}}
					\vspace{0.1in}
					\item $f(x) = x^{\tfrac{1}{3}}$, $\delta = \tfrac{1}{3}$, and $r = 0.11$.
					\vspace{0.1in}
					\item The explosive solutions are ruled out without directly imposing the boundary condition.
					\vspace{0.1in}
					\item Very accurate approximations, both in the short- and medium-run.
					\vspace{0.1in}
					\item Learns the  \emphcolor{correct steady state}. \hyperlink{errors}{\beamerskipbutton{Relative errors}}
				\end{itemize}
		\end{column}
		\begin{column}{0.37\textwidth}
			\begin{figure}[t!]
				\centering
				\includegraphics[width=\textwidth]{figs/neoclassical_growth_model_baseline.pdf}
				\vspace{-7mm}
			\end{figure}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Neoclassical growth model: learning the steady state}
	
	Why does it learn the \emphcolor{correct steady state}?
	
	\begin{itemize}
		\item A straight line is the ``smoothest'' solution: it has zero derivatives.
		
		\item The kernels we use are zero-reverting:
		\[
		\lim_{t \rightarrow \infty} k(t,t_j) = 0.
		\]
		
		\item We approximate the derivatives using kernels:
		\[
		\lim_{t \rightarrow \infty} \hat{\dot{x}}(t)
		= \hat{\dot{\mu}}(t)
		= \hat{\dot{y}}(t)
		= 0.
		\]
		
		\item This behavior is mainly driven by choosing a large $t_N$ (e.g., $t_N = 40$) in $\mathcal{D}$.
		\vspace{0.1in}
		
		\item \emphcolor{Question}: How accurate are the short-run dynamics when $t_N$ is small?
	\end{itemize}	
\end{frame}

\begin{frame}{Neoclassical growth model: short-run results}
	\label{ncg-short-run-results}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item $\mathcal{D} = \{0,1,\cdots,10\}$.
				\vspace{0.1in}
				
				\item $f(x) = x^{\tfrac{1}{3}}$, $\delta = \tfrac{1}{3}$, and $r = 0.11$.
				\vspace{0.1in}
				
				\item Very accurate short-run dynamics.
				\vspace{0.1in}
				\hyperlink{errors-short-run}{\beamerskipbutton{Relative errors}}
				
				\vspace{0.1in}
				\emphcolor{Conclusion}
				\begin{itemize}
					\item For short-run accuracy, we do not need to know the steady state (global condition).
					\vspace{0.1in}
					
					\item It suffices that sub-optimal paths diverge fast enough from the optimal path (local condition).
				\end{itemize}
			\end{itemize}
		\end{column}
		
		\begin{column}{0.37\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth]{figs/neoclassical_growth_model_short_run.pdf}
				\vspace{-7mm}
			\end{figure}
		\end{column}
	\end{columns}
\end{frame}





%\begin{frame}{Short-run planning: ``In the long run, we are all dead" }
%	\begin{figure}[t!]
		% previously we chose a long horizon for time in the training data, what happens we focus on a very short time planning? 
%		\centering
%		\includegraphics[width=\textwidth]{figs/neoclassical_growth_short_planning.pdf}
%		\caption*{$\mathcal{D} = \{0,1,\cdots,10\}$}
%		\vspace{-4mm}
%	\end{figure}
%	\begin{itemize}
%		\item The explosive solutions are ruled out without %		\vspace{0.1in}
%		\item Provides a very accurate approximation in the short-run.
%	\end{itemize}
%\end{frame}

\section{Extensions}

\begin{frame}{Neoclassical Growth Model: Non-Concave Production Function}
	\begin{itemize}
		\item So far we have had a \emphcolor{unique} saddle-path converging to a unique \emphcolor{saddle} steady state.
		\vspace{0.1in}
		\item What if we have \emphcolor{two} saddle steady states, very close to each other (steady-state multiplicity)?
		\vspace{0.1in}
		\item Neoclassical growth model with a non-concave production function (threshold externalities):
		\begin{align*}
			f(x) = A \max\{x^a, b_1x^a - b_2\}
		\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Non-concave production function: vector field}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{align*}
				&\dot{\mathbf{x}}(t) = f(\mathbf{x}(t)) - \mathbf{y}(t)-\delta \mathbf{x}(t)\\
				&\dot{\mathbf{y}}(t) = \mathbf{y}(t)\big[f'(\mathbf{x}(t)) -\delta -r\big]\\
				& \mathbf{x}(0) = \mathbf{x}_0 ~ \text{given}.
			\end{align*}
		\end{column}
		\begin{column}{0.5\textwidth}
			\begin{figure}[t!]
				\centering
				\includegraphics[width=\textwidth]{figs/vec_field_con_con.pdf}
				\vspace{-4mm}
			\end{figure}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Results}
	\label{butterfly}
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.8\textwidth]{figs/neoclassical_growth_model_concave_convex_threshold.pdf}
		\vspace{-4mm}
	\end{figure}
	\begin{itemize}
		\item The approximate solutions approach the right steady states.
		\vspace{0.1in}
		\item The transversality conditions are satisfied without being directly imposed.
		\vspace{0.1in}
		\item The steady states are learned. 
		\hyperlink{DAE}{\beamerskipbutton{Full DAE}}
	\end{itemize}
\end{frame}

\begin{frame}{Linear asset pricing}
	%why this probelm? Not because this problem is hard. But that is the first model that is the simplest model that incorporates bubbles in economics
	Linear asset pricing model
	\[ 
	\begin{aligned}
		\dot{x}(t) &= c + g x(t)\\
		\dot{\mu}(t) &= r \mu(t) - x(t) := r \mu(t) - \mu(t) \frac{x(t)}{\mu(t)}\\
		0 &= \lim_{t\rightarrow \infty} e^{-r t}\mu(t)x(t).
	\end{aligned}
	\]
	\begin{itemize}
		\item $x(t)\in \mathbb{R}$: flow payoffs from a claim to an asset. 
		\vspace{0.1in}
		\item $\mu(t) \in \R$ be the price of a claim to that asset.
		\vspace{0.1in}
		\item $x_0$ given. 
	\end{itemize}
\end{frame}

\begin{frame}{Why do we need the boundary condition?}
	%Imagine if I didnt have the boundary condition, what would happen?
	\begin{align*}
		\dot{\mathbf{x}}(t) &= c + g \mathbf{x}(t) \\
		\dot{\mathbf{y}}(t) &= r \mathbf{y}(t) - \mathbf{x}(t)
	\end{align*}
	\begin{itemize}
		\item The solutions: 
		\begin{align*}
			\mathbf{y}(t) = \mathbf{y}_f(t) + \zeta e^{rt}
		\end{align*}
		\item $\mathbf{y}_f(t) = \int_0^\infty e^{-r\tau} \mathbf{x}(t+s)ds$: price based on the fundamentals.
		\vspace{0.1in}
		\item $\zeta e^{rt}$: explosive bubble terms, it has to be \emphcolor{ruled out} by the boundary condition.
		%doesnt correspond to any economic variable
		\vspace{0.1in} 
		\item Triangle inequality: $\Vert \mathbf{y}_f\Vert < \Vert \mathbf{y}\Vert$.
		%go with any norm, because of the positivity and addtive structure 
		\vspace{0.1in}
		\item The price based on the fundamentals has the \emphcolor{lowest norm}.
	\end{itemize}
\end{frame}

\begin{frame}{Results}
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.8\textwidth]{figs/asset_pricing_contiguous.pdf}
		\caption*{$\mathcal{D} = \{0,1,\cdots,30\}$}
		\vspace{-4mm}
	\end{figure}
	\begin{itemize}
		\item The explosive solutions are ruled out without directly imposing the boundary condition.
		\vspace{0.1in}
		\item Very accurate approximations, both in the short- and medium-run.
		\vspace{0.1in}
		\item Learns the steady-state.
		% By learning I mean, the function stays on a flat line, outside of the training data
	\end{itemize}
\end{frame}



\begin{frame}{Conclusion}
	\begin{itemize}
		\item Long-run (\emphcolor{global}) conditions can be replaced with appropriate regularization (\emphcolor{local}) to achieve the optimal solutions.
		\vspace{0.1in}
		\item The minimum-norm implicit bias of large ML models aligns with optimality in economic dynamic models.
		\vspace{0.1in}
		\item Both kernel and neural network approximations accurately learn the right steady state(s).
		\vspace{0.1in}
		\item Proceeding with \emphcolor{caution}: can regularization be thought of as an equilibrium selection device?
		%if we are about to translate economic dynamic models into an ERM, can regularization act as an equilibrium device? Think about it, ERM in statistical learning or machine learning is about two things: uniform of law of large numbers and capacity control (so what are going to do about this capacity control regularization)
	\end{itemize}
\end{frame}

\section{Appendix}

\begin{frame}{Matérn kernel}
	\label{Maten-def}
		\begin{align*}
		K(t, t_j) = C_{\frac{1}{2}}(t,t_j) = \sigma^2 \exp\left(-\frac{|t - t_j|}{\ell}\right),
	\end{align*}
	\begin{align*}
		K(t, t_j) = C_{\frac{3}{2}}(t,t_j) = \sigma^2 \left(1 + \frac{\sqrt{3} |t - t_j|}{\ell}\right) \exp\left(-\frac{\sqrt{3} |t - t_j|}{\ell}\right),
	\end{align*}
	\begin{align*}
		K(t, t_j) =  C_{\frac{5}{2}}(t,t_j) =  \sigma^2 \left(1 + \frac{\sqrt{5} |t - t_j|}{\ell} + \frac{5 |t - t_j|^2}{3 \ell^2}\right) \exp\left(-\frac{\sqrt{5} |t - t_j|}{\ell}\right).
	\end{align*}
	\hyperlink{kernel-approax}{\beamerskipbutton{Back}}
\end{frame}

\begin{frame}{Neoclassical growth: relative errors}
	\label{errors}
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.7\textwidth]{figs/neoclassical_growth_model_baseline_rel_err.pdf}
		\vspace{-4mm}
	\end{figure}
			\hyperlink{ncg-results}{\beamerskipbutton{Back}}
\end{frame}

\begin{frame}{Neoclassical growth: short-run relative errors}
	\label{errors-short-run}
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.7\textwidth]{figs/neoclassical_growth_model_short_run_full.pdf}
		\vspace{-4mm}
	\end{figure}
	\hyperlink{ncg-short-run-results}{\beamerskipbutton{Back}}
\end{frame}


\begin{frame}{Neoclassical growth: sparse sampling}
	\label{sparse-sampling}
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.6\textwidth]{figs/neoclassical_growth_model_sparse.pdf}
		\vspace{-4mm}
	\end{figure}
	\[
	\begin{aligned}
		\mathcal{D} = \{0, 1, 3, 5, 10, 15, 20, 25, 30, 35, 38, 40\}
	\end{aligned}
	\]
	\hyperlink{ncg-results}{\beamerskipbutton{Back}}
\end{frame}

\begin{frame}{Human capital and growth}
	\label{DAE}
\begin{align*}
	\dot{\mathbf{x}}_k(t) & = \mathbf{y}_{k}(t)-\delta_k \mathbf{x}_k(t),\\
	\dot{\mathbf{x}}_h(t) & = \mathbf{y}_{h}(t)-\delta_k \mathbf{x}_h(t)\\
	\dot{\mathbf{y}}_c(t) &= \mathbf{y}_c(t) \left[ f_1\left(\mathbf{x}_k(t),\mathbf{x}_h(t)\right) -\delta_k - r \right],\\
	0 & =   f\left(\mathbf{x}_k(t),\mathbf{x}_h(t)\right) - \mathbf{y}_c(t) -  \mathbf{y}_{k}(t) -  \mathbf{y}_{h}(t), \label{eq:human-capital-feasibility-condition}\\
	0 & = f_2\left(\mathbf{x}_k(t),\mathbf{x}_h(t)\right) - f_1\left(\mathbf{x}_k(t),\mathbf{x}_h(t)\right) + \delta_k-\delta_h.\\
		0 & = \lim_{t\rightarrow \infty} e^{-r t}\frac{\mathbf{x}_k(t)}{\mathbf{y}_c(t)}, ~
	0 = \lim_{t\rightarrow \infty} e^{-r t}\frac{\mathbf{x}_h(t)}{\mathbf{y}_c(t)}.
\end{align*}


\begin{itemize}
	\item $\mathbf{x}_k$: physical capital,  $\mathbf{x}_h:$ human capital, $\mathbf{y}_c$: consumption, $\mathbf{y}_k$: investment in physical capital, $\mathbf{y}_h$: investment in human capital 
	\vspace{0.1in}
	\item $f\left(x_k, x_h\right) = x_k^{a_k}x_h^{a_h}$
\end{itemize}
\hyperlink{butterfly}{\beamerskipbutton{Back}}
\end{frame}

\begin{frame}{Results}
		\begin{figure}[t!]
		\centering
		\includegraphics[width=0.5\textwidth]{figs/human_capital.pdf}
		\vspace{-4mm}
	\end{figure}
\end{frame}
\end{document}
